{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitur yang dipakai :\n",
    "- TFIDF Word\n",
    "- Question Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T22:51:03.650942Z",
     "start_time": "2020-05-17T22:51:03.542934Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import re \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T00:14:48.455110Z",
     "start_time": "2020-05-18T00:14:48.436111Z"
    }
   },
   "outputs": [],
   "source": [
    "sen_class = 1\n",
    "\n",
    "target = 'dataset-question-classification-csv.csv'\n",
    "with open(target,'r',encoding='utf-8') as csvFile:\n",
    "    csvReader = csv.reader(csvFile, delimiter=';')\n",
    "    next(csvReader)\n",
    "    \n",
    "    text = []\n",
    "    classes = []\n",
    "    question_length = []\n",
    "    \n",
    "    for row in csvReader:\n",
    "        \n",
    "        sentence = row[0].lower()\n",
    "        \n",
    "        if sen_class == 1:\n",
    "            sentenceclass =row[1] #row[1]=coarse - row[2]=fine\n",
    "        elif sen_class == 2:\n",
    "            sentenceclass =row[2] #row[1]=coarse - row[2]=fine\n",
    "        classes.append(sentenceclass)\n",
    "        \n",
    "        #Question length\n",
    "        q_length = len(re.findall(r'\\w+', sentence)) \n",
    "        question_length.append(q_length)\n",
    "        \n",
    "        text.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T00:14:49.230323Z",
     "start_time": "2020-05-18T00:14:48.907305Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_feature = tfidf_vectorizer.fit_transform(text)\n",
    "documents_text = tfidf_feature.toarray()\n",
    "documents_text = documents_text.tolist()\n",
    "\n",
    "# Insert Question Length\n",
    "i=0\n",
    "for xxxxx in documents_text:\n",
    "    documents_text[i].insert(0,question_length[i])\n",
    "    i+=1\n",
    "    \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "a_train, a_test, b_train, b_test = train_test_split(documents_text, classes, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T00:15:01.797278Z",
     "start_time": "2020-05-18T00:14:51.326926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR      1.000     0.857     0.923        21\n",
      "        DESC      0.844     0.792     0.817        48\n",
      "        ENTY      0.855     0.958     0.903       166\n",
      "         HUM      0.989     0.931     0.959       101\n",
      "         LOC      0.902     0.833     0.866        66\n",
      "         NUM      1.000     0.979     0.989       143\n",
      "\n",
      "    accuracy                          0.925       545\n",
      "   macro avg      0.932     0.892     0.910       545\n",
      "weighted avg      0.928     0.925     0.925       545\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM model Coarse Category\n",
    "from sklearn import svm\n",
    "model = svm.SVC(kernel='linear')\n",
    "model.fit(a_train,b_train)\n",
    "\n",
    "predictions = model.predict(a_test)\n",
    "print(classification_report(b_test,predictions,digits=3))\n",
    "print('\\n\\n')\n",
    "\n",
    "pickle.dump(model, open(\"newest-model-coarse.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T00:13:28.765186Z",
     "start_time": "2020-05-18T00:13:14.654141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "  abbreviation      0.833     1.000     0.909         5\n",
      "        animal      0.846     0.786     0.815        14\n",
      "          body      0.833     0.833     0.833         6\n",
      "          city      1.000     0.750     0.857         8\n",
      "          code      1.000     1.000     1.000         4\n",
      "         color      1.000     0.800     0.889        10\n",
      "         count      0.659     0.829     0.734        35\n",
      "       country      0.750     1.000     0.857        12\n",
      "      creation      0.846     0.786     0.815        14\n",
      "      currency      1.000     0.800     0.889         5\n",
      "          date      0.875     0.933     0.903        15\n",
      "    definition      0.368     1.000     0.538         7\n",
      "   description      0.600     0.375     0.462         8\n",
      "        dismed      1.000     0.250     0.400        12\n",
      "      distance      0.917     0.917     0.917        12\n",
      "     entyother      0.667     0.267     0.381        15\n",
      "         event      0.286     0.500     0.364         4\n",
      "     expansion      1.000     0.750     0.857        16\n",
      "          food      1.000     0.462     0.632        13\n",
      "         group      0.750     0.900     0.818        10\n",
      "humdescription      1.000     0.857     0.923        14\n",
      "    individual      0.759     0.985     0.857        67\n",
      "    instrument      1.000     1.000     1.000         1\n",
      "      language      1.000     1.000     1.000         2\n",
      "        letter      0.000     0.000     0.000         3\n",
      "      locother      0.500     0.886     0.639        35\n",
      "        manner      0.900     0.750     0.818        12\n",
      "         money      0.706     0.889     0.787        27\n",
      "      mountain      1.000     0.500     0.667         6\n",
      "      numother      1.000     0.200     0.333         5\n",
      "         order      0.500     0.500     0.500         2\n",
      "       percent      0.000     0.000     0.000         8\n",
      "        period      0.900     0.818     0.857        11\n",
      "         plant      1.000     0.250     0.400         4\n",
      "       product      1.000     0.273     0.429        11\n",
      "        reason      0.704     0.905     0.792        21\n",
      "      religion      0.000     0.000     0.000         2\n",
      "          size      1.000     1.000     1.000         5\n",
      "         speed      1.000     0.333     0.500        12\n",
      "         sport      0.900     1.000     0.947         9\n",
      "         state      0.800     0.800     0.800         5\n",
      "     substance      0.600     1.000     0.750         3\n",
      "        symbol      1.000     1.000     1.000         5\n",
      "     technique      1.000     0.800     0.889         5\n",
      "   temperature      0.000     0.000     0.000         1\n",
      "          term      0.889     0.727     0.800        11\n",
      "         title      1.000     0.700     0.824        10\n",
      "       vehicle      0.667     0.571     0.615         7\n",
      "        weight      1.000     0.833     0.909         6\n",
      "          word      1.000     0.700     0.824        10\n",
      "\n",
      "      accuracy                          0.758       545\n",
      "     macro avg      0.781     0.684     0.695       545\n",
      "  weighted avg      0.791     0.758     0.739       545\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\fandy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#SVM model Fine Category\n",
    "from sklearn import svm\n",
    "model = svm.SVC(kernel='linear')\n",
    "model.fit(a_train,b_train)\n",
    "\n",
    "predictions = model.predict(a_test)\n",
    "print(classification_report(b_test,predictions,digits=3))\n",
    "print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
