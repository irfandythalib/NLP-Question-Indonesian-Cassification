{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitur yang dipakai :\n",
    "- TFIDF Word\n",
    "- WOrd Shape\n",
    "- Question Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T01:44:08.680571Z",
     "start_time": "2020-09-08T01:43:46.080564Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "import re \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T01:44:09.233566Z",
     "start_time": "2020-09-08T01:44:08.683569Z"
    }
   },
   "outputs": [],
   "source": [
    "sen_class = 1\n",
    "\n",
    "target = 'dataset-question-classification-csv.csv'\n",
    "with open(target,'r',encoding='utf-8') as csvFile:\n",
    "    csvReader = csv.reader(csvFile, delimiter=';')\n",
    "    next(csvReader)\n",
    "    \n",
    "    text = []\n",
    "    classes = []\n",
    "    question_length = []\n",
    "    w_shape_upper = []\n",
    "    w_shape_lower = []\n",
    "    w_shape_digit = []\n",
    "    w_shape_mix = []\n",
    "    w_shape_other = []\n",
    "    \n",
    "    for row in csvReader:\n",
    "        \n",
    "        sentence = row[0].lower()\n",
    "        \n",
    "        if sen_class == 1:\n",
    "            sentenceclass =row[1] #row[1]=coarse - row[2]=fine\n",
    "        elif sen_class == 2:\n",
    "            sentenceclass =row[2] #row[1]=coarse - row[2]=fine\n",
    "        classes.append(sentenceclass)\n",
    "        \n",
    "        #Word Shape\n",
    "        shape_upper = 0\n",
    "        shape_lower = 0\n",
    "        shape_digit = 0\n",
    "        shape_mix = 0\n",
    "        shape_other = 0\n",
    "        words = word_tokenize(sentence)\n",
    "        islower = len([word for word in words if word.islower()])\n",
    "        shape_lower += islower\n",
    "        isupper = len([word for word in words if word.isupper()])\n",
    "        shape_upper += isupper\n",
    "        isdigit = len([word for word in words if word.isdigit()])\n",
    "        shape_digit += isdigit\n",
    "        a = [word for word in words if not word.islower() and not word.isupper() and not word.isdigit()]\n",
    "        for x in a:\n",
    "            if x.isalpha() == True:\n",
    "                shape_mix += 1\n",
    "            if x.isalpha() == False:\n",
    "                shape_other+=1\n",
    "        w_shape_upper.append(shape_upper)\n",
    "        w_shape_lower.append(shape_lower)\n",
    "        w_shape_digit.append(shape_digit)\n",
    "        w_shape_mix.append(shape_mix)\n",
    "        w_shape_other.append(shape_other)\n",
    "        \n",
    "        #Question length\n",
    "        q_length = len(re.findall(r'\\w+', sentence)) \n",
    "        question_length.append(q_length)\n",
    "        \n",
    "        text.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T01:44:09.787372Z",
     "start_time": "2020-09-08T01:44:09.235569Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "#Eksport TFIDF model\n",
    "tfidf_vectorizer.fit(text)\n",
    "\n",
    "#Eksekusi TFIDF seperti biasanya\n",
    "tfidf_feature = tfidf_vectorizer.transform(text)\n",
    "documents_text = tfidf_feature.toarray()\n",
    "documents_text = documents_text.tolist()\n",
    "\n",
    "\n",
    "# Insert Word Shape\n",
    "i=0\n",
    "for xxxxx in documents_text:\n",
    "    documents_text[i].insert(0,w_shape_upper[i])\n",
    "    i+=1\n",
    "i=0\n",
    "for xxxxx in documents_text:\n",
    "    documents_text[i].insert(0,w_shape_lower[i])\n",
    "    i+=1\n",
    "i=0\n",
    "for xxxxx in documents_text:\n",
    "    documents_text[i].insert(0,w_shape_digit[i])\n",
    "    i+=1\n",
    "i=0\n",
    "for xxxxx in documents_text:\n",
    "    documents_text[i].insert(0,w_shape_mix[i])\n",
    "    i+=1\n",
    "i=0\n",
    "for xxxxx in documents_text:\n",
    "    documents_text[i].insert(0,w_shape_other[i])\n",
    "    i+=1\n",
    "\n",
    "# Insert Question Length\n",
    "i=0\n",
    "for xxxxx in documents_text:\n",
    "    documents_text[i].insert(0,question_length[i])\n",
    "    i+=1\n",
    "    \n",
    "    \n",
    "    \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "a_train, a_test, b_train, b_test = train_test_split(documents_text, classes, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T01:44:10.624576Z",
     "start_time": "2020-09-08T01:44:10.615572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2977"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T00:15:21.923771Z",
     "start_time": "2020-05-18T00:15:11.352990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR      1.000     0.857     0.923        21\n",
      "        DESC      0.826     0.792     0.809        48\n",
      "        ENTY      0.859     0.952     0.903       166\n",
      "         HUM      0.989     0.931     0.959       101\n",
      "         LOC      0.903     0.848     0.875        66\n",
      "         NUM      1.000     0.979     0.989       143\n",
      "\n",
      "    accuracy                          0.925       545\n",
      "   macro avg      0.930     0.893     0.910       545\n",
      "weighted avg      0.928     0.925     0.925       545\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM model Coarse Category\n",
    "from sklearn import svm\n",
    "model = svm.SVC(kernel='linear')\n",
    "model.fit(a_train,b_train)\n",
    "\n",
    "predictions = model.predict(a_test)\n",
    "print(classification_report(b_test,predictions,digits=3))\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T00:13:53.284561Z",
     "start_time": "2020-05-18T00:13:39.216531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "  abbreviation      0.833     1.000     0.909         5\n",
      "        animal      0.846     0.786     0.815        14\n",
      "          body      0.833     0.833     0.833         6\n",
      "          city      1.000     0.750     0.857         8\n",
      "          code      1.000     1.000     1.000         4\n",
      "         color      1.000     0.800     0.889        10\n",
      "         count      0.674     0.829     0.744        35\n",
      "       country      0.733     0.917     0.815        12\n",
      "      creation      0.769     0.714     0.741        14\n",
      "      currency      1.000     0.800     0.889         5\n",
      "          date      0.933     0.933     0.933        15\n",
      "    definition      0.333     1.000     0.500         7\n",
      "   description      0.429     0.375     0.400         8\n",
      "        dismed      1.000     0.250     0.400        12\n",
      "      distance      0.917     0.917     0.917        12\n",
      "     entyother      0.667     0.267     0.381        15\n",
      "         event      0.375     0.750     0.500         4\n",
      "     expansion      1.000     0.750     0.857        16\n",
      "          food      0.750     0.462     0.571        13\n",
      "         group      0.800     0.800     0.800        10\n",
      "humdescription      1.000     0.857     0.923        14\n",
      "    individual      0.759     0.985     0.857        67\n",
      "    instrument      1.000     1.000     1.000         1\n",
      "      language      1.000     0.500     0.667         2\n",
      "        letter      0.000     0.000     0.000         3\n",
      "      locother      0.517     0.886     0.653        35\n",
      "        manner      0.900     0.750     0.818        12\n",
      "         money      0.750     0.889     0.814        27\n",
      "      mountain      1.000     0.500     0.667         6\n",
      "      numother      1.000     0.200     0.333         5\n",
      "         order      0.000     0.000     0.000         2\n",
      "       percent      0.000     0.000     0.000         8\n",
      "        period      0.818     0.818     0.818        11\n",
      "         plant      1.000     0.250     0.400         4\n",
      "       product      1.000     0.364     0.533        11\n",
      "        reason      0.704     0.905     0.792        21\n",
      "      religion      0.000     0.000     0.000         2\n",
      "          size      1.000     1.000     1.000         5\n",
      "         speed      1.000     0.333     0.500        12\n",
      "         sport      0.875     0.778     0.824         9\n",
      "         state      0.800     0.800     0.800         5\n",
      "     substance      0.600     1.000     0.750         3\n",
      "        symbol      1.000     1.000     1.000         5\n",
      "     technique      1.000     0.800     0.889         5\n",
      "   temperature      0.000     0.000     0.000         1\n",
      "          term      0.778     0.636     0.700        11\n",
      "         title      1.000     0.700     0.824        10\n",
      "       vehicle      0.556     0.714     0.625         7\n",
      "        weight      1.000     0.833     0.909         6\n",
      "          word      0.875     0.700     0.778        10\n",
      "\n",
      "      accuracy                          0.749       545\n",
      "     macro avg      0.756     0.663     0.672       545\n",
      "  weighted avg      0.778     0.749     0.731       545\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\fandy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#SVM model Fine Category\n",
    "from sklearn import svm\n",
    "model = svm.SVC(kernel='linear')\n",
    "model.fit(a_train,b_train)\n",
    "\n",
    "predictions = model.predict(a_test)\n",
    "print(classification_report(b_test,predictions,digits=3))\n",
    "print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
